# -*- coding: utf-8 -*-
"""BERT classifier

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gTeW-icju1IcmwoB4gTckq1YH0sibJ2H
"""

!pip3 install transformers numpy torch sklearn

!pip install tensorflow

import torch
from transformers import AutoModel, AutoTokenizer
import tensorflow_datasets as tfds
import numpy as np
from transformers import BertTokenizerFast, BertForSequenceClassification
from transformers import Trainer, TrainingArguments

imdb,info=tfds.load('imdb_reviews',with_info=True,as_supervised=True)

train_data,test_data=imdb['train'], imdb['test']

import string
import re

def standardize_data(comment):
    table = str.maketrans('', '', string.punctuation)
    comment= [comment.translate(table)]
    comment[0] = re.sub(r"[\.,\?]+$-", "", comment[0])
    comment[0] = re.sub(" \d+", " ", comment[0])
    return comment[0]

train_comment=[]
test_comment=[]
train_label=[]
test_label=[]

for c,l in train_data:
  c=standardize_data(str(c))
  train_comment.append(c)
  train_label.append(l.numpy())

for c,l in test_data:
  c=standardize_data(str(c))
  test_comment.append(c)
  test_label.append(l.numpy())

max_length = 512

custokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased",do_lower_case=True)

train_encodings = custokenizer(train_comment, truncation=True, padding=True, max_length=max_length)
test_encodings = custokenizer(test_comment, truncation=True, padding=True, max_length=max_length)

class GroupsDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}
        item["labels"] = torch.tensor([self.labels[idx]])
        return item

    def __len__(self):
        return len(self.labels)

# convert our tokenized data into a torch Dataset
train_dataset = GroupsDataset(train_encodings, train_label)
test_dataset = GroupsDataset(test_encodings, test_label)

model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)

from sklearn.metrics import f1_score
def compute_metrics(pred):
  labels = pred.label_ids
  preds = pred.predictions.argmax(-1)
  # calculate accuracy using sklearn's function
  f1 = f1_score(labels, preds)
  return {
      'accuracy': f1,
  }

training_args = TrainingArguments(
    output_dir='./results',          
    num_train_epochs=10,              
    per_device_train_batch_size=8,  
    per_device_eval_batch_size=8,   
    warmup_steps=500,                
    weight_decay=0.01,               
    logging_dir='./logs',            
    load_best_model_at_end=True,     
    logging_steps=500,               
    evaluation_strategy="steps",     
)

trainer = Trainer(
    model=model,                         
    args=training_args,                  
    train_dataset=train_dataset,         
    eval_dataset=test_dataset,          
    compute_metrics=compute_metrics,     
)

trainer.train()

def get_prediction(text):
    # prepare our text into tokenized sequence
    inputs = tokenizer(text, padding=True, truncation=True, max_length=max_length, return_tensors="pt")
    # perform inference to our model
    outputs = model(**inputs)
    # get output probabilities by doing softmax
    probs = outputs[0].softmax(1)
    # executing argmax function to get the candidate label
    return probs.argmax()
